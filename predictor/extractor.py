"""Feature extraction utilities for SMAAF's malware predictor.

The helpers defined here mirror the feature set used by the original
training pipeline so that previously generated datasets remain compatible.
"""
from __future__ import annotations

import logging
import os
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import capstone
import numpy as np
import pefile

LOGGER = logging.getLogger(__name__)


def _safe_entropy(data: bytes) -> float:
    """Return the Shannon entropy for *data* with deterministic semantics."""

    if not data:
        return 0.0

    counts = np.bincount(np.frombuffer(data, dtype=np.uint8), minlength=256)
    total = len(data)
    probs = counts[counts > 0] / float(total)
    entropy = -np.sum(probs * np.log2(probs))
    return float(entropy)


def _section_name(section: pefile.SectionStructure) -> str:
    return section.Name.decode(errors="ignore").strip("\x00")


class PEFeatureExtractor:
    """Extracts a feature dictionary from a PE file.

    The output mirrors the JSON shape generated by
    ``hydradragon/machine_learning/train.py`` so that SMAAF can train and serve
    models that behave identically to the reference implementation.
    """

    def __init__(self) -> None:
        self._md32 = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_32)
        self._md64 = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_64)

    # ------------------------------------------------------------------
    # Disassembly helpers
    # ------------------------------------------------------------------
    def _capstone_for(self, pe: pefile.PE) -> Optional[capstone.Cs]:
        machine = pe.FILE_HEADER.Machine
        if machine == pefile.MACHINE_TYPE["IMAGE_FILE_MACHINE_I386"]:
            return self._md32
        if machine == pefile.MACHINE_TYPE["IMAGE_FILE_MACHINE_AMD64"]:
            return self._md64
        LOGGER.warning("Unsupported machine type for Capstone analysis: %s", machine)
        return None

    def disassemble_all_sections(self, pe: pefile.PE) -> Dict[str, Any]:
        result: Dict[str, Any] = {
            "overall_analysis": {
                "total_instructions": 0,
                "add_count": 0,
                "mov_count": 0,
                "is_likely_packed": False,
            },
            "sections": {},
            "error": None,
        }

        md = self._capstone_for(pe)
        if md is None:
            result["error"] = "Unsupported architecture"
            return result

        total_add = 0
        total_mov = 0
        total_instr = 0

        for section in pe.sections:
            name = _section_name(section)
            code = section.get_data()
            base = pe.OPTIONAL_HEADER.ImageBase + section.VirtualAddress

            if not code:
                result["sections"][name] = {
                    "instruction_counts": {},
                    "total_instructions": 0,
                    "add_count": 0,
                    "mov_count": 0,
                    "is_likely_packed": False,
                }
                continue

            instruction_counts: Dict[str, int] = {}
            total_section_instr = 0

            try:
                for ins in md.disasm(code, base):
                    mnemonic = ins.mnemonic
                    instruction_counts[mnemonic] = instruction_counts.get(mnemonic, 0) + 1
                    total_section_instr += 1
            except Exception as exc:  # pragma: no cover - defensive
                LOGGER.exception("Capstone disassembly failed for %s: %s", name, exc)
                result["sections"][name] = {
                    "instruction_counts": {},
                    "total_instructions": 0,
                    "add_count": 0,
                    "mov_count": 0,
                    "is_likely_packed": False,
                }
                continue

            add_count = instruction_counts.get("add", 0)
            mov_count = instruction_counts.get("mov", 0)

            total_add += add_count
            total_mov += mov_count
            total_instr += total_section_instr

            result["sections"][name] = {
                "instruction_counts": instruction_counts,
                "total_instructions": total_section_instr,
                "add_count": add_count,
                "mov_count": mov_count,
                "is_likely_packed": bool(total_section_instr and add_count > mov_count),
            }

        result["overall_analysis"] = {
            "total_instructions": total_instr,
            "add_count": total_add,
            "mov_count": total_mov,
            "is_likely_packed": bool(total_instr and total_add > total_mov),
        }
        return result

    # ------------------------------------------------------------------
    # Extraction helpers mirroring the legacy training implementation
    # ------------------------------------------------------------------
    def extract_section_data(self, section: pefile.SectionStructure) -> Dict[str, Any]:
        raw = section.get_data()
        return {
            "name": _section_name(section),
            "virtual_size": section.Misc_VirtualSize,
            "virtual_address": section.VirtualAddress,
            "size_of_raw_data": section.SizeOfRawData,
            "pointer_to_raw_data": section.PointerToRawData,
            "characteristics": section.Characteristics,
            "entropy": _safe_entropy(raw),
            "raw_data_size": len(raw) if raw else 0,
        }

    def extract_imports(self, pe: pefile.PE) -> List[Dict[str, Any]]:
        imports: List[Dict[str, Any]] = []
        if not hasattr(pe, "DIRECTORY_ENTRY_IMPORT"):
            return imports
        for entry in pe.DIRECTORY_ENTRY_IMPORT:
            imports.append(
                {
                    "dll_name": entry.dll.decode(errors="ignore") if entry.dll else None,
                    "imports": [
                        {
                            "name": imp.name.decode(errors="ignore") if imp.name else None,
                            "address": imp.address,
                            "ordinal": imp.ordinal,
                        }
                        for imp in entry.imports
                    ],
                }
            )
        return imports

    def extract_exports(self, pe: pefile.PE) -> List[Dict[str, Any]]:
        exports: List[Dict[str, Any]] = []
        if hasattr(pe, "DIRECTORY_ENTRY_EXPORT"):
            for exp in pe.DIRECTORY_ENTRY_EXPORT.symbols:
                exports.append(
                    {
                        "name": exp.name.decode(errors="ignore") if exp.name else None,
                        "address": exp.address,
                        "ordinal": exp.ordinal,
                        "forwarder": exp.forwarder.decode(errors="ignore") if exp.forwarder else None,
                    }
                )
        return exports

    def _get_tls_callback_addresses(self, pe: pefile.PE, address: int) -> List[int]:
        callbacks: List[int] = []
        try:
            while True:
                rva = address - pe.OPTIONAL_HEADER.ImageBase
                value = pe.get_dword_at_rva(rva)
                if value == 0:
                    break
                callbacks.append(value)
                address += 4
        except Exception as exc:  # pragma: no cover - defensive
            LOGGER.debug("TLS callback enumeration failed: %s", exc)
        return callbacks

    def analyze_tls_callbacks(self, pe: pefile.PE) -> Dict[str, Any]:
        if not hasattr(pe, "DIRECTORY_ENTRY_TLS"):
            return {}
        tls = pe.DIRECTORY_ENTRY_TLS.struct
        info: Dict[str, Any] = {
            "start_address_raw_data": tls.StartAddressOfRawData,
            "end_address_raw_data": tls.EndAddressOfRawData,
            "address_of_index": tls.AddressOfIndex,
            "address_of_callbacks": tls.AddressOfCallBacks,
            "size_of_zero_fill": tls.SizeOfZeroFill,
            "characteristics": tls.Characteristics,
            "callbacks": [],
        }
        if tls.AddressOfCallBacks:
            info["callbacks"] = self._get_tls_callback_addresses(pe, tls.AddressOfCallBacks)
        return info

    def analyze_dos_stub(self, pe: pefile.PE) -> Dict[str, Any]:
        result = {"exists": False, "size": 0, "entropy": 0.0}
        try:
            if hasattr(pe, "DOS_HEADER"):
                stub_offset = pe.DOS_HEADER.e_lfanew - 64
                if stub_offset > 0:
                    data = pe.__data__[64:pe.DOS_HEADER.e_lfanew]
                    if data:
                        result["exists"] = True
                        result["size"] = len(data)
                        result["entropy"] = _safe_entropy(data)
        except Exception as exc:  # pragma: no cover - defensive
            LOGGER.debug("DOS stub analysis failed: %s", exc)
        return result

    def analyze_certificates(self, pe: pefile.PE) -> Dict[str, Any]:
        info: Dict[str, Any] = {}
        try:
            if hasattr(pe, "DIRECTORY_ENTRY_SECURITY"):
                info["virtual_address"] = pe.DIRECTORY_ENTRY_SECURITY.VirtualAddress
                info["size"] = pe.DIRECTORY_ENTRY_SECURITY.Size
            if hasattr(pe, "VS_FIXEDFILEINFO"):
                fixed = pe.VS_FIXEDFILEINFO
                info["fixed_file_info"] = {
                    "signature": fixed.Signature,
                    "struct_version": fixed.StrucVersion,
                    "file_version": f"{fixed.FileVersionMS >> 16}.{fixed.FileVersionMS & 0xFFFF}.{fixed.FileVersionLS >> 16}.{fixed.FileVersionLS & 0xFFFF}",
                    "product_version": f"{fixed.ProductVersionMS >> 16}.{fixed.ProductVersionMS & 0xFFFF}.{fixed.ProductVersionLS >> 16}.{fixed.ProductVersionLS & 0xFFFF}",
                    "file_flags": fixed.FileFlags,
                    "file_os": fixed.FileOS,
                    "file_type": fixed.FileType,
                    "file_subtype": fixed.FileSubtype,
                }
        except Exception as exc:  # pragma: no cover - defensive
            LOGGER.debug("Certificate analysis failed: %s", exc)
        return info

    def analyze_delay_imports(self, pe: pefile.PE) -> List[Dict[str, Any]]:
        result: List[Dict[str, Any]] = []
        if not hasattr(pe, "DIRECTORY_ENTRY_DELAY_IMPORT"):
            return result
        for entry in pe.DIRECTORY_ENTRY_DELAY_IMPORT:
            imports = []
            for imp in entry.imports:
                imports.append(
                    {
                        "name": imp.name.decode(errors="ignore") if imp.name else None,
                        "address": imp.address,
                        "ordinal": imp.ordinal,
                    }
                )
            result.append(
                {
                    "dll": entry.dll.decode(errors="ignore") if entry.dll else None,
                    "attributes": getattr(entry.struct, "Attributes", None),
                    "name": getattr(entry.struct, "Name", None),
                    "handle": getattr(entry.struct, "Handle", None),
                    "iat": getattr(entry.struct, "IAT", None),
                    "bound_iat": getattr(entry.struct, "BoundIAT", None),
                    "unload_iat": getattr(entry.struct, "UnloadIAT", None),
                    "timestamp": getattr(entry.struct, "TimeDateStamp", None),
                    "imports": imports,
                }
            )
        return result

    def analyze_load_config(self, pe: pefile.PE) -> Dict[str, Any]:
        if not hasattr(pe, "DIRECTORY_ENTRY_LOAD_CONFIG"):
            return {}
        cfg = pe.DIRECTORY_ENTRY_LOAD_CONFIG.struct
        return {
            "size": cfg.Size,
            "timestamp": cfg.TimeDateStamp,
            "major_version": cfg.MajorVersion,
            "minor_version": cfg.MinorVersion,
            "global_flags_clear": cfg.GlobalFlagsClear,
            "global_flags_set": cfg.GlobalFlagsSet,
            "critical_section_default_timeout": cfg.CriticalSectionDefaultTimeout,
            "decommit_free_block_threshold": cfg.DeCommitFreeBlockThreshold,
            "decommit_total_free_threshold": cfg.DeCommitTotalFreeThreshold,
            "security_cookie": cfg.SecurityCookie,
            "se_handler_table": cfg.SEHandlerTable,
            "se_handler_count": cfg.SEHandlerCount,
        }

    def analyze_relocations(self, pe: pefile.PE) -> List[Dict[str, Any]]:
        result: List[Dict[str, Any]] = []
        if not hasattr(pe, "DIRECTORY_ENTRY_BASERELOC"):
            return result
        for base in pe.DIRECTORY_ENTRY_BASERELOC:
            entry_types: Dict[int, int] = {}
            offsets: List[int] = []
            for entry in base.entries:
                entry_types[entry.type] = entry_types.get(entry.type, 0) + 1
                offsets.append(entry.rva - base.struct.VirtualAddress)
            result.append(
                {
                    "virtual_address": base.struct.VirtualAddress,
                    "size_of_block": base.struct.SizeOfBlock,
                    "summary": {
                        "total_entries": len(base.entries),
                        "types": entry_types,
                        "offset_range": (min(offsets), max(offsets)) if offsets else None,
                    },
                }
            )
        return result

    def analyze_bound_imports(self, pe: pefile.PE) -> List[Dict[str, Any]]:
        result: List[Dict[str, Any]] = []
        if not hasattr(pe, "DIRECTORY_ENTRY_BOUND_IMPORT"):
            return result
        for bound in pe.DIRECTORY_ENTRY_BOUND_IMPORT:
            entry = {
                "name": bound.name.decode(errors="ignore") if bound.name else None,
                "timestamp": bound.struct.TimeDateStamp,
                "references": [],
            }
            if hasattr(bound, "references") and bound.references:
                for ref in bound.references:
                    entry["references"].append(
                        {
                            "name": ref.name.decode(errors="ignore") if ref.name else None,
                            "timestamp": getattr(ref.struct, "TimeDateStamp", None),
                        }
                    )
            result.append(entry)
        return result

    def analyze_section_characteristics(self, pe: pefile.PE) -> Dict[str, Dict[str, Any]]:
        characteristics: Dict[str, Dict[str, Any]] = {}
        for section in pe.sections:
            flags = section.Characteristics
            characteristics[_section_name(section)] = {
                "flags": {
                    "CODE": bool(flags & 0x20),
                    "INITIALIZED_DATA": bool(flags & 0x40),
                    "UNINITIALIZED_DATA": bool(flags & 0x80),
                    "MEM_DISCARDABLE": bool(flags & 0x2000000),
                    "MEM_NOT_CACHED": bool(flags & 0x4000000),
                    "MEM_NOT_PAGED": bool(flags & 0x8000000),
                    "MEM_SHARED": bool(flags & 0x10000000),
                    "MEM_EXECUTE": bool(flags & 0x20000000),
                    "MEM_READ": bool(flags & 0x40000000),
                    "MEM_WRITE": bool(flags & 0x80000000),
                },
                "entropy": _safe_entropy(section.get_data()),
                "size_ratio": (
                    section.SizeOfRawData / pe.OPTIONAL_HEADER.SizeOfImage
                    if pe.OPTIONAL_HEADER.SizeOfImage
                    else 0
                ),
                "pointer_to_raw_data": section.PointerToRawData,
                "pointer_to_relocations": section.PointerToRelocations,
                "pointer_to_line_numbers": section.PointerToLinenumbers,
                "number_of_relocations": section.NumberOfRelocations,
                "number_of_line_numbers": section.NumberOfLinenumbers,
            }
        return characteristics

    def analyze_extended_headers(self, pe: pefile.PE) -> Dict[str, Any]:
        headers: Dict[str, Any] = {
            "dos_header": {},
            "nt_headers": {},
        }
        try:
            dos = pe.DOS_HEADER
            headers["dos_header"] = {
                "e_magic": dos.e_magic,
                "e_cblp": dos.e_cblp,
                "e_cp": dos.e_cp,
                "e_crlc": dos.e_crlc,
                "e_cparhdr": dos.e_cparhdr,
                "e_minalloc": dos.e_minalloc,
                "e_maxalloc": dos.e_maxalloc,
                "e_ss": dos.e_ss,
                "e_sp": dos.e_sp,
                "e_csum": dos.e_csum,
                "e_ip": dos.e_ip,
                "e_cs": dos.e_cs,
                "e_lfarlc": dos.e_lfarlc,
                "e_ovno": dos.e_ovno,
                "e_oemid": dos.e_oemid,
                "e_oeminfo": dos.e_oeminfo,
            }
        except Exception as exc:  # pragma: no cover - defensive
            LOGGER.debug("DOS header parsing failed: %s", exc)

        if hasattr(pe, "NT_HEADERS") and pe.NT_HEADERS is not None:
            nt = pe.NT_HEADERS
            if hasattr(nt, "FileHeader"):
                headers["nt_headers"] = {
                    "signature": nt.Signature,
                    "machine": nt.FileHeader.Machine,
                    "number_of_sections": nt.FileHeader.NumberOfSections,
                    "time_date_stamp": nt.FileHeader.TimeDateStamp,
                    "characteristics": nt.FileHeader.Characteristics,
                }
        return headers

    def serialize_data(self, value: Any) -> Optional[List[int]]:
        try:
            return list(value) if value else None
        except Exception:  # pragma: no cover - defensive
            return None

    def analyze_rich_header(self, pe: pefile.PE) -> Dict[str, Any]:
        if not hasattr(pe, "RICH_HEADER") or pe.RICH_HEADER is None:
            return {}
        rich = pe.RICH_HEADER
        result: Dict[str, Any] = {
            "checksum": getattr(rich, "checksum", None),
            "values": self.serialize_data(rich.values),
            "clear_data": self.serialize_data(rich.clear_data),
            "key": self.serialize_data(rich.key),
            "raw_data": self.serialize_data(rich.raw_data),
            "comp_id_info": [],
        }
        values = result["values"] or []
        for i in range(0, len(values), 2):
            if i + 1 >= len(values):
                break
            comp_id = values[i] >> 16
            build_number = values[i] & 0xFFFF
            count = values[i + 1]
            result["comp_id_info"].append(
                {
                    "comp_id": comp_id,
                    "build_number": build_number,
                    "count": count,
                }
            )
        return result

    def analyze_overlay(self, pe: pefile.PE, file_path: str) -> Dict[str, Any]:
        info = {"exists": False, "offset": 0, "size": 0, "entropy": 0.0}
        try:
            if not pe.sections:
                return info
            last = max(pe.sections, key=lambda s: s.PointerToRawData + s.SizeOfRawData)
            end_of_pe = last.PointerToRawData + last.SizeOfRawData
            file_size = os.path.getsize(file_path)
            if file_size > end_of_pe:
                with open(file_path, "rb") as handle:
                    handle.seek(end_of_pe)
                    overlay = handle.read()
                info.update(
                    {
                        "exists": True,
                        "offset": end_of_pe,
                        "size": len(overlay),
                        "entropy": _safe_entropy(overlay),
                    }
                )
        except Exception as exc:  # pragma: no cover - defensive
            LOGGER.debug("Overlay analysis failed for %s: %s", file_path, exc)
        return info

    def extract_numeric_features(self, file_path: str, rank: Optional[int] = None) -> Optional[Dict[str, Any]]:
        pe = None
        try:
            pe = pefile.PE(file_path, fast_load=True)
        except pefile.PEFormatError:
            LOGGER.error("%s is not a valid PE file", file_path)
            return None
        except Exception as exc:  # pragma: no cover - defensive
            LOGGER.exception("Failed to load PE %s: %s", file_path, exc)
            return None

        try:
            try:
                pe.parse_data_directories()
            except Exception:  # pragma: no cover - best effort
                LOGGER.debug("parse_data_directories failed for %s", file_path, exc_info=True)

            features: Dict[str, Any] = {
                "section_disassembly": self.disassemble_all_sections(pe),
                "SizeOfOptionalHeader": pe.FILE_HEADER.SizeOfOptionalHeader,
                "MajorLinkerVersion": pe.OPTIONAL_HEADER.MajorLinkerVersion,
                "MinorLinkerVersion": pe.OPTIONAL_HEADER.MinorLinkerVersion,
                "SizeOfCode": pe.OPTIONAL_HEADER.SizeOfCode,
                "SizeOfInitializedData": pe.OPTIONAL_HEADER.SizeOfInitializedData,
                "SizeOfUninitializedData": pe.OPTIONAL_HEADER.SizeOfUninitializedData,
                "AddressOfEntryPoint": pe.OPTIONAL_HEADER.AddressOfEntryPoint,
                "BaseOfCode": pe.OPTIONAL_HEADER.BaseOfCode,
                "BaseOfData": getattr(pe.OPTIONAL_HEADER, "BaseOfData", 0),
                "ImageBase": pe.OPTIONAL_HEADER.ImageBase,
                "SectionAlignment": pe.OPTIONAL_HEADER.SectionAlignment,
                "FileAlignment": pe.OPTIONAL_HEADER.FileAlignment,
                "MajorOperatingSystemVersion": pe.OPTIONAL_HEADER.MajorOperatingSystemVersion,
                "MinorOperatingSystemVersion": pe.OPTIONAL_HEADER.MinorOperatingSystemVersion,
                "MajorImageVersion": pe.OPTIONAL_HEADER.MajorImageVersion,
                "MinorImageVersion": pe.OPTIONAL_HEADER.MinorImageVersion,
                "MajorSubsystemVersion": pe.OPTIONAL_HEADER.MajorSubsystemVersion,
                "MinorSubsystemVersion": pe.OPTIONAL_HEADER.MinorSubsystemVersion,
                "SizeOfImage": pe.OPTIONAL_HEADER.SizeOfImage,
                "SizeOfHeaders": pe.OPTIONAL_HEADER.SizeOfHeaders,
                "CheckSum": pe.OPTIONAL_HEADER.CheckSum,
                "Subsystem": pe.OPTIONAL_HEADER.Subsystem,
                "DllCharacteristics": pe.OPTIONAL_HEADER.DllCharacteristics,
                "SizeOfStackReserve": pe.OPTIONAL_HEADER.SizeOfStackReserve,
                "SizeOfStackCommit": pe.OPTIONAL_HEADER.SizeOfStackCommit,
                "SizeOfHeapReserve": pe.OPTIONAL_HEADER.SizeOfHeapReserve,
                "SizeOfHeapCommit": pe.OPTIONAL_HEADER.SizeOfHeapCommit,
                "LoaderFlags": pe.OPTIONAL_HEADER.LoaderFlags,
                "NumberOfRvaAndSizes": pe.OPTIONAL_HEADER.NumberOfRvaAndSizes,
                "sections": [self.extract_section_data(section) for section in pe.sections],
                "imports": self.extract_imports(pe),
                "exports": self.extract_exports(pe),
                "resources": self._extract_resources(pe),
                "debug": self._extract_debug(pe),
                "certificates": self.analyze_certificates(pe),
                "dos_stub": self.analyze_dos_stub(pe),
                "tls_callbacks": self.analyze_tls_callbacks(pe),
                "delay_imports": self.analyze_delay_imports(pe),
                "load_config": self.analyze_load_config(pe),
                "bound_imports": self.analyze_bound_imports(pe),
                "section_characteristics": self.analyze_section_characteristics(pe),
                "extended_headers": self.analyze_extended_headers(pe),
                "rich_header": self.analyze_rich_header(pe),
                "overlay": self.analyze_overlay(pe, file_path),
                "relocations": self.analyze_relocations(pe),
            }
            if rank is not None:
                features["numeric_tag"] = rank
            return features
        finally:
            if pe is not None:
                try:
                    pe.close()
                except Exception:  # pragma: no cover - defensive
                    LOGGER.debug("Failed to close PE handle for %s", file_path, exc_info=True)

    # ------------------------------------------------------------------
    # Misc helpers mirroring the legacy tooling
    # ------------------------------------------------------------------
    def _extract_resources(self, pe: pefile.PE) -> List[Dict[str, Any]]:
        resources: List[Dict[str, Any]] = []
        if not hasattr(pe, "DIRECTORY_ENTRY_RESOURCE"):
            return resources
        try:
            for resource_type in pe.DIRECTORY_ENTRY_RESOURCE.entries:
                directory = getattr(resource_type, "directory", None)
                if directory is None:
                    continue
                for resource_id in directory.entries:
                    sub_dir = getattr(resource_id, "directory", None)
                    if sub_dir is None:
                        continue
                    for resource_lang in sub_dir.entries:
                        data = getattr(resource_lang, "data", None)
                        resources.append(
                            {
                                "type_id": getattr(resource_type.struct, "Id", None),
                                "resource_id": getattr(resource_id.struct, "Id", None),
                                "lang_id": getattr(resource_lang.struct, "Id", None),
                                "size": getattr(data, "Size", None) if data else None,
                                "codepage": getattr(data, "CodePage", None) if data else None,
                            }
                        )
        except Exception as exc:  # pragma: no cover - defensive
            LOGGER.debug("Resource parsing failed: %s", exc)
        return resources

    def _extract_debug(self, pe: pefile.PE) -> List[Dict[str, Any]]:
        entries: List[Dict[str, Any]] = []
        if not hasattr(pe, "DIRECTORY_ENTRY_DEBUG"):
            return entries
        for debug in pe.DIRECTORY_ENTRY_DEBUG:
            try:
                entries.append(
                    {
                        "type": debug.struct.Type,
                        "timestamp": debug.struct.TimeDateStamp,
                        "version": f"{debug.struct.MajorVersion}.{debug.struct.MinorVersion}",
                        "size": debug.struct.SizeOfData,
                    }
                )
            except Exception as exc:  # pragma: no cover - defensive
                LOGGER.debug("Debug directory parsing failed: %s", exc)
        return entries


def features_to_vector(entry: Dict[str, Any]) -> np.ndarray:
    """Convert the legacy feature dictionary to a numeric vector.

    The mapping reproduces the historic ``features_to_numeric`` function so
    that previously trained models can be refreshed without schema drift.
    """

    def to_float(value: Any, default: float = 0.0) -> float:
        try:
            if value is None:
                return float(default)
            return float(value)
        except Exception:
            return float(default)

    def safe_len(value: Any) -> int:
        try:
            return len(value) if value is not None else 0
        except Exception:
            return 0

    def section_entropy_stats(section_characteristics: Any) -> Tuple[float, float, float]:
        entropies: List[float] = []
        if isinstance(section_characteristics, dict):
            for data in section_characteristics.values():
                if isinstance(data, dict):
                    entropy = data.get("entropy")
                    if entropy is not None:
                        try:
                            entropies.append(float(entropy))
                        except Exception:
                            continue
        if not entropies:
            return 0.0, 0.0, 0.0
        mean = sum(entropies) / len(entropies)
        return float(mean), float(min(entropies)), float(max(entropies))

    def reloc_summary(relocs: Any) -> Tuple[int, int]:
        total = 0
        blocks = 0
        if isinstance(relocs, list):
            for reloc in relocs:
                blocks += 1
                try:
                    total += int(reloc.get("summary", {}).get("total_entries", 0))
                except Exception:
                    continue
        return total, blocks

    entry = entry or {}

    size_of_optional_header = to_float(entry.get("SizeOfOptionalHeader", 0))
    major_linker = to_float(entry.get("MajorLinkerVersion", 0))
    minor_linker = to_float(entry.get("MinorLinkerVersion", 0))
    size_of_code = to_float(entry.get("SizeOfCode", 0))
    size_of_init_data = to_float(entry.get("SizeOfInitializedData", 0))
    size_of_uninit_data = to_float(entry.get("SizeOfUninitializedData", 0))
    address_of_entry = to_float(entry.get("AddressOfEntryPoint", 0))
    image_base = to_float(entry.get("ImageBase", 0))
    subsystem = to_float(entry.get("Subsystem", 0))
    dll_characteristics = to_float(entry.get("DllCharacteristics", 0))
    size_of_stack_reserve = to_float(entry.get("SizeOfStackReserve", 0))
    size_of_heap_reserve = to_float(entry.get("SizeOfHeapReserve", 0))
    checksum = to_float(entry.get("CheckSum", 0))
    num_rva_and_sizes = to_float(entry.get("NumberOfRvaAndSizes", 0))
    size_of_image = to_float(entry.get("SizeOfImage", 0))

    imports_count = safe_len(entry.get("imports", []))
    exports_count = safe_len(entry.get("exports", []))
    resources_count = safe_len(entry.get("resources", []))
    sections_count = safe_len(entry.get("sections", []))

    overlay = entry.get("overlay", {}) or {}
    overlay_exists = int(bool(overlay.get("exists")))
    overlay_size = to_float(overlay.get("size", 0))

    sec_char = entry.get("section_characteristics", {}) or {}
    sec_entropy_mean, sec_entropy_min, sec_entropy_max = section_entropy_stats(sec_char)

    sec_disasm = entry.get("section_disassembly", {}) or {}
    overall = sec_disasm.get("overall_analysis", {}) or {}
    total_instructions = to_float(overall.get("total_instructions", 0))
    total_adds = to_float(overall.get("add_count", 0))
    total_movs = to_float(overall.get("mov_count", 0))
    is_likely_packed = int(bool(overall.get("is_likely_packed")))

    add_mov_ratio = total_adds / (total_movs + 1.0)
    instrs_per_kb = total_instructions / ((size_of_image / 1024.0) + 1e-6)

    tls = entry.get("tls_callbacks", {}) or {}
    tls_callbacks_list = tls.get("callbacks", []) if isinstance(tls, dict) else []
    num_tls_callbacks = safe_len(tls_callbacks_list)

    delay_imports_list = entry.get("delay_imports", []) or []
    num_delay_imports = safe_len(delay_imports_list)

    relocs = entry.get("relocations", []) or []
    num_reloc_entries, num_reloc_blocks = reloc_summary(relocs)

    bound_imports = entry.get("bound_imports", []) or []
    num_bound_imports = safe_len(bound_imports)

    debug_entries = entry.get("debug", []) or []
    num_debug_entries = safe_len(debug_entries)
    cert_info = entry.get("certificates", {}) or {}
    cert_size = to_float(cert_info.get("size", 0))

    rich_header = entry.get("rich_header", {}) or {}
    has_rich = int(bool(rich_header))

    vector = np.array(
        [
            size_of_optional_header,
            major_linker,
            minor_linker,
            size_of_code,
            size_of_init_data,
            size_of_uninit_data,
            address_of_entry,
            image_base,
            subsystem,
            dll_characteristics,
            size_of_stack_reserve,
            size_of_heap_reserve,
            checksum,
            num_rva_and_sizes,
            size_of_image,
            float(imports_count),
            float(exports_count),
            float(resources_count),
            float(overlay_exists),
            float(sections_count),
            float(sec_entropy_mean),
            float(sec_entropy_min),
            float(sec_entropy_max),
            float(total_instructions),
            float(total_adds),
            float(total_movs),
            float(is_likely_packed),
            float(add_mov_ratio),
            float(instrs_per_kb),
            float(overlay_size),
            float(num_tls_callbacks),
            float(num_delay_imports),
            float(num_reloc_entries),
            float(num_reloc_blocks),
            float(num_bound_imports),
            float(num_debug_entries),
            float(cert_size),
            float(has_rich),
        ],
        dtype=np.float32,
    )
    return vector


def feature_vector_from_path(path: Path) -> Optional[np.ndarray]:
    extractor = PEFeatureExtractor()
    features = extractor.extract_numeric_features(str(path))
    if not features:
        return None
    return features_to_vector(features)
